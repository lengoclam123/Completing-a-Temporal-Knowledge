{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This notebook use for comparing 3 algorithm:\n","    - Graph Convolution Neural Network-Diachronic Embedding-SimplE\n","    - Diachronic Embedding-SimplE\n","    - Diachronic Embedding-TransE\n","## Structure:\n","    Support object:\n","        - Scripts\n","        - Params \n","        - Measure \n","    Machine learning format:\n","        - Dataset\n","        - tester\n","        - trainer\n","    Model (User can modify another Graph of anotehr Diachronic)\n","        - G_de_simple:Graph Convolution Neural Network-Diachronic Embedding-SimplE\n","        - G_simple: Graph Convolution Neural Network-SimplE\n","        - DE-simple: Diachronic Embedding-SimplE\n","        - DE-TransE: Diachronic Embedding-TransE\n","## How to use:\n","    1. Upload this notebook and data follow it to kaggle\n","    2. Change hyperparameter in block main.py\n","    3. Access to main.py to run or change hyperparameter\n","\n","    A few notes:\n","        - Every model parameter save in \"/kaggle/working/models \"\n","        - Information about each run (include both train and test) save in \"/kaggle/working/exps\"\n","    \n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:43.056232Z","iopub.status.busy":"2024-10-21T20:28:43.055406Z","iopub.status.idle":"2024-10-21T20:28:43.063794Z","shell.execute_reply":"2024-10-21T20:28:43.062647Z","shell.execute_reply.started":"2024-10-21T20:28:43.056181Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["cd /kaggle/working/ # switch to kaggle/working to save model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","import math\n","import copy\n","import time\n","import numpy as np\n","from random import shuffle\n","from datetime import datetime\n","import time\n","import contextlib"]},{"cell_type":"markdown","metadata":{},"source":["# Scripts.py"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.575768Z","iopub.status.busy":"2024-10-21T20:28:47.575342Z","iopub.status.idle":"2024-10-21T20:28:47.583619Z","shell.execute_reply":"2024-10-21T20:28:47.582569Z","shell.execute_reply.started":"2024-10-21T20:28:47.575733Z"},"trusted":true},"outputs":[],"source":["def shredFacts(facts): #takes a batch of facts and shreds it into its columns\n","        \n","    heads      = torch.tensor(facts[:,0]).long().cuda()\n","    rels       = torch.tensor(facts[:,1]).long().cuda()\n","    tails      = torch.tensor(facts[:,2]).long().cuda()\n","    years = torch.tensor(facts[:,3]).float().cuda()\n","    months = torch.tensor(facts[:,4]).float().cuda()\n","    days = torch.tensor(facts[:,5]).float().cuda()\n","    return heads, rels, tails, years, months, days\n"]},{"cell_type":"markdown","metadata":{},"source":["# Params.py"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.587196Z","iopub.status.busy":"2024-10-21T20:28:47.586557Z","iopub.status.idle":"2024-10-21T20:28:47.600636Z","shell.execute_reply":"2024-10-21T20:28:47.599814Z","shell.execute_reply.started":"2024-10-21T20:28:47.587148Z"},"trusted":true},"outputs":[],"source":["class Params:\n","\n","    def __init__(self, \n","                 ne=500, \n","                 bsize=512, \n","                 lr=0.001, \n","                 reg_lambda=0.0, \n","                 emb_dim=100, \n","                 neg_ratio=20, \n","                 dropout=0.4,  \n","                 save_each=50,  \n","                 se_prop=0.9\n","                ):\n","\n","        self.ne = ne\n","        self.bsize = bsize\n","        self.lr = lr\n","        self.reg_lambda = reg_lambda\n","        self.s_emb_dim = int(se_prop*emb_dim)\n","        self.t_emb_dim = emb_dim - int(se_prop*emb_dim)\n","        self.save_each = save_each\n","        self.neg_ratio = neg_ratio\n","        self.dropout = dropout\n","        self.se_prop = se_prop\n","        self.emb_dim = emb_dim\n","        \n","    def str_(self):\n","        return str(self.ne) + \"_\" + str(self.bsize) + \"_\" + str(self.lr) + \"_\" + str(self.reg_lambda) + \"_\" + str(self.s_emb_dim) + \"_\" + str(self.neg_ratio) + \"_\" + str(self.dropout) + \"_\" + str(self.t_emb_dim) + \"_\" + str(self.save_each) + \"_\" + str(self.se_prop) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Measure.py"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.602623Z","iopub.status.busy":"2024-10-21T20:28:47.602221Z","iopub.status.idle":"2024-10-21T20:28:47.617924Z","shell.execute_reply":"2024-10-21T20:28:47.616924Z","shell.execute_reply.started":"2024-10-21T20:28:47.602582Z"},"trusted":true},"outputs":[],"source":["\n","class Measure:\n","    \n","    def __init__(self):\n","        self.hit1  = {\"raw\": 0.0, \"fil\": 0.0}\n","        self.hit3  = {\"raw\": 0.0, \"fil\": 0.0}\n","        self.hit10 = {\"raw\": 0.0, \"fil\": 0.0}\n","        self.mrr   = {\"raw\": 0.0, \"fil\": 0.0}\n","        self.mr    = {\"raw\": 0.0, \"fil\": 0.0}\n","        \n","    def update(self, rank, raw_or_fil):\n","        if rank == 1:\n","            self.hit1[raw_or_fil] += 1.0\n","        if rank <= 3:\n","            self.hit3[raw_or_fil] += 1.0\n","        if rank <= 10:\n","            self.hit10[raw_or_fil] += 1.0\n","            \n","        self.mr[raw_or_fil]  += rank\n","        self.mrr[raw_or_fil] += (1.0 / rank)\n","        \n","    def normalize(self, num_facts):\n","        for raw_or_fil in [\"raw\", \"fil\"]:\n","            self.hit1[raw_or_fil]  /= (2 * num_facts)\n","            self.hit3[raw_or_fil]  /= (2 * num_facts)\n","            self.hit10[raw_or_fil] /= (2 * num_facts)\n","            self.mr[raw_or_fil]    /= (2 * num_facts)\n","            self.mrr[raw_or_fil]   /= (2 * num_facts)\n","            \n","    def print_(self):\n","\n","        for raw_or_fil in [\"raw\", \"fil\"]:\n","            print(raw_or_fil.title() + \" setting:\")\n","            print(\"\\tHit@1 =\",  self.hit1[raw_or_fil])\n","            print(\"\\tHit@3 =\",  self.hit3[raw_or_fil])\n","            print(\"\\tHit@10 =\", self.hit10[raw_or_fil])\n","            print(\"\\tMR =\",     self.mr[raw_or_fil])\n","            print(\"\\tMRR =\",    self.mrr[raw_or_fil])\n","            print(\"\")\n","    def print__(self):\n","        for raw_or_fil in [\"fil\"]:\n","            print(raw_or_fil.title() + \" setting:\")\n","            print(\"\")\n","            print(\"Hit@1 =\",  self.hit1[raw_or_fil])\n","            print(\"Hit@3 =\",  self.hit3[raw_or_fil])\n","            print(\"Hit@10 =\", self.hit10[raw_or_fil])\n","            print(\"MR =\",     self.mr[raw_or_fil])\n","            print(\"MRR =\",    self.mrr[raw_or_fil])\n","            "]},{"cell_type":"markdown","metadata":{},"source":["# Dataset.py"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.620812Z","iopub.status.busy":"2024-10-21T20:28:47.620386Z","iopub.status.idle":"2024-10-21T20:28:47.660001Z","shell.execute_reply":"2024-10-21T20:28:47.659040Z","shell.execute_reply.started":"2024-10-21T20:28:47.620761Z"},"trusted":true},"outputs":[],"source":["class Dataset:\n","    \"\"\"Implements the specified dataloader\"\"\"\n","    def __init__(self, ds_name):\n","        \"\"\"\n","        Params: ds_name : name of the dataset \n","        \"\"\"\n","        self.name = ds_name\n","        self.ds_path = ds_name\n","        self.ent2id = {}\n","        self.rel2id = {}\n","        self.data = {\"train\": self.readFile(self.ds_path + \"train.txt\"),\n","                     \"valid\": self.readFile(self.ds_path + \"valid.txt\"),\n","                     \"test\":  self.readFile(self.ds_path + \"test.txt\")}\n","        \n","        self.start_batch = 0\n","        self.all_facts_as_tuples = None\n","        \n","        self.convertTimes()\n","        \n","        self.all_facts_as_tuples = set([tuple(d) for d in self.data[\"train\"] + self.data[\"valid\"] + self.data[\"test\"]])\n","        \n","        for spl in [\"train\", \"valid\", \"test\"]:\n","            self.data[spl] = np.array(self.data[spl])\n","        \n","    def readFile(self, \n","                 filename):\n","        #Input : path_file\n","        #Output: list facts\n","            #facts[0] = [1,2,3,2014-06-27]\n","      \n","        with open(filename, \"r\",encoding='utf-8') as f:\n","            data = f.readlines()\n","        \n","        facts = []\n","        for line in data:\n","            elements = line.strip().split(\"\\t\")\n","            \n","            head_id =  self.getEntID(elements[0])\n","            rel_id  =  self.getRelID(elements[1])\n","            tail_id =  self.getEntID(elements[2])\n","            timestamp = elements[3]\n","            \n","            facts.append([head_id, rel_id, tail_id, timestamp])\n","            \n","        return facts\n","    \n","    \n","    def convertTimes(self):      \n","        \"\"\"\n","        This function spits the timestamp in the day,date and time.\n","        \"\"\"  \n","        for split in [\"train\", \"valid\", \"test\"]:\n","            for i, fact in enumerate(self.data[split]):\n","                fact_date = fact[-1]\n","                self.data[split][i] = self.data[split][i][:-1]\n","                date = list(map(float, fact_date.split(\"-\")))\n","                self.data[split][i] += date\n","        #Change data :\n","            # Exp: 2014-06-27 --> [2014,6,27]\n","                \n","                \n","    \n","    def numEnt(self): \n","    \n","        return len(self.ent2id)\n","\n","    def numRel(self): \n","    \n","        return len(self.rel2id)\n","\n","    \n","    def getEntID(self,ent_name):\n","\n","        if ent_name in self.ent2id:\n","            return self.ent2id[ent_name] \n","        self.ent2id[ent_name] = len(self.ent2id)\n","        return self.ent2id[ent_name]\n","    \n","    def getRelID(self, rel_name):\n","        if rel_name in self.rel2id:\n","            return self.rel2id[rel_name] \n","        self.rel2id[rel_name] = len(self.rel2id)\n","        return self.rel2id[rel_name]\n","\n","    \n","    def nextPosBatch(self, batch_size): \n","        #Input: batch size\n","        #Output:\n","            # ret_facts.shape = [batch_size x features]\n","            # ret_facts[0] = [head,relation,tail,year,month,day] \n","        if self.start_batch + batch_size > len(self.data[\"train\"]):\n","            ret_facts = self.data[\"train\"][self.start_batch : ]\n","            self.start_batch = 0\n","        else:\n","            ret_facts = self.data[\"train\"][self.start_batch : self.start_batch + batch_size]\n","            self.start_batch += batch_size\n","\n","\n","        return ret_facts\n","    \n","\n","    def addNegFacts(self, bp_facts, neg_ratio):\n","        ex_per_pos = 2 * neg_ratio + 2\n","        facts = np.repeat(np.copy(bp_facts), ex_per_pos, axis=0)\n","        for i in range(bp_facts.shape[0]):\n","            s1 = i * ex_per_pos + 1\n","            e1 = s1 + neg_ratio\n","            s2 = e1 + 1\n","            e2 = s2 + neg_ratio\n","            \n","            facts[s1:e1,0] = (facts[s1:e1,0] + np.random.randint(low=1, high=self.numEnt(), size=neg_ratio)) % self.numEnt()\n","            facts[s2:e2,2] = (facts[s2:e2,2] + np.random.randint(low=1, high=self.numEnt(), size=neg_ratio)) % self.numEnt()\n","            \n","        return facts\n","    \n","    def addNegFacts2(self, bp_facts, neg_ratio):\n","        # input bp_facts, neg_ratio\n","            # bp_facts.shape = [batch_size x features]\n","        # output A = concat(facts1,facts2)\n","        \n","            # A.shape = [batch_size x (neg_ratio + 1)] x [features]\n","            # A = [[1,2,3,4,5,6],\n","            #      . . . \n","            #      [1,2,3,4,5,6]]\n","        \n","        pos_neg_group_size = 1 + neg_ratio\n","        facts1 = np.repeat(np.copy(bp_facts), pos_neg_group_size, axis=0)\n","        facts2 = np.copy(facts1)\n","        rand_nums1 = np.random.randint(low=1, high=self.numEnt(), size=facts1.shape[0])\n","        rand_nums2 = np.random.randint(low=1, high=self.numEnt(), size=facts2.shape[0])\n","        \n","        for i in range(facts1.shape[0] // pos_neg_group_size):\n","            rand_nums1[i * pos_neg_group_size] = 0\n","            rand_nums2[i * pos_neg_group_size] = 0\n","        \n","        facts1[:,0] = (facts1[:,0] + rand_nums1) % self.numEnt()\n","        facts2[:,2] = (facts2[:,2] + rand_nums2) % self.numEnt()\n","\n","   \n","        return np.concatenate((facts1, facts2), axis=0)\n","\n","  \n","    \n","    def nextBatch(self, batch_size, neg_ratio=1):\n","        bp_facts = self.nextPosBatch(batch_size)\n","        batch = shredFacts(self.addNegFacts2(bp_facts, neg_ratio)) #=>  batch = 6 tensors, tensor 1 = heads, tensor 2 = rels , . . . \n","        alldata = torch.tensor(self.data['train']).float() # => alldata.shape = [number_instance x features]\n","       \n","        return  batch, alldata #batch = (heads, rels, tails, years, months, day) \n","    \n","    \n","    def wasLastBatch(self):\n","        return (self.start_batch == 0)"]},{"cell_type":"markdown","metadata":{},"source":["# Graph representation "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.661902Z","iopub.status.busy":"2024-10-21T20:28:47.661593Z","iopub.status.idle":"2024-10-21T20:28:47.677832Z","shell.execute_reply":"2024-10-21T20:28:47.677072Z","shell.execute_reply.started":"2024-10-21T20:28:47.661869Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class GraphConvolution(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(GraphConvolution, self).__init__()\n","        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n","        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n","        nn.init.xavier_uniform_(self.weight)\n","        nn.init.zeros_(self.bias)\n","\n","    def forward(self, node_features, adj):\n","      \n","        support = torch.mm(node_features, self.weight)  # [num_entities, out_features]\n","        output = torch.mm(adj, support)  # Aggretage function [num_entities, out_features]\n","        output = output + self.bias  # add bias\n","        return output\n","\n","\n","class GraphRepresentation(nn.Module):\n","    def __init__(self, num_entities, num_relations, t_emb_dim):\n","        super(GraphRepresentation, self).__init__()\n","        self.num_entities = num_entities\n","        self.num_relations = num_relations\n","        self.t_emb_dim = t_emb_dim\n","\n","        self.entity_embeddings = nn.Embedding(num_entities, t_emb_dim)\n","        \n","        self.graph_conv1 = GraphConvolution(t_emb_dim, t_emb_dim)\n","        self.graph_conv2 = GraphConvolution(t_emb_dim, t_emb_dim)\n","\n","    def forward(self, input_tensor, adj):\n","\n","        heads = input_tensor[:, 0].long()  \n","        rels = input_tensor[:, 1].long()   \n","        tails = input_tensor[:, 2].long()  \n","\n","        entity_features = self.entity_embeddings.weight  #  [num_entities, t_emb_dim]\n","\n","        x = self.graph_conv1(entity_features, adj)  # [num_entities, t_emb_dim]\n","        x = F.relu(x)\n","        x = self.graph_conv2(x, adj)  # [num_entities, t_emb_dim]\n","        \n","\n","        heads_embedding = x[heads]  #  [batch_size, t_emb_dim]\n","     \n","        return heads_embedding\n"]},{"cell_type":"markdown","metadata":{},"source":["# G_de_simple.py"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.726272Z","iopub.status.busy":"2024-10-21T20:28:47.725915Z","iopub.status.idle":"2024-10-21T20:28:47.775395Z","shell.execute_reply":"2024-10-21T20:28:47.774461Z","shell.execute_reply.started":"2024-10-21T20:28:47.726239Z"},"trusted":true},"outputs":[],"source":["\n","\n","class G_DE_SimplE(torch.nn.Module):\n","    def __init__(self, dataset, params):\n","        super(G_DE_SimplE, self).__init__()\n","        self.dataset = dataset\n","        self.params = params\n","\n","        self.s_emb_dim = params.s_emb_dim\n","        self.g_emb_dim = params.s_emb_dim\n","        self.t_emb_dim = params.emb_dim - self.s_emb_dim - self.g_emb_dim\n","        \n","        self.ent_embs_h = nn.Embedding(dataset.numEnt(),  self.s_emb_dim).cuda()\n","        self.ent_embs_t = nn.Embedding(dataset.numEnt(),  self.s_emb_dim).cuda()\n","        self.rel_embs_f = nn.Embedding(dataset.numRel(), self.s_emb_dim + self.g_emb_dim + self.t_emb_dim).cuda()\n","        self.rel_embs_i = nn.Embedding(dataset.numRel(), self.s_emb_dim + self.g_emb_dim + self.t_emb_dim).cuda()\n","        \n","        self.create_time_embedds()\n","\n","        self.graphRepresent = GraphRepresentation(dataset.numEnt(), dataset.numRel(), self.g_emb_dim)\n","\n","        self.time_nl = torch.sin\n","        \n","        nn.init.xavier_uniform_(self.ent_embs_h.weight)\n","        nn.init.xavier_uniform_(self.ent_embs_t.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_f.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_i.weight)\n","    \n","    def create_time_embedds(self):\n","\n","        # frequency embeddings for the entities\n","        self.m_freq_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.m_freq_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_freq_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_freq_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_freq_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_freq_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","\n","        # phi embeddings for the entities\n","        self.m_phi_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.m_phi_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_phi_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_phi_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_phi_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_phi_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","\n","        # frequency embeddings for the entities\n","        self.m_amps_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.m_amps_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_amps_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.d_amps_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_amps_h = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        self.y_amps_t = nn.Embedding(self.dataset.numEnt(), self.t_emb_dim).cuda()\n","        nn.init.xavier_uniform_(self.m_freq_h.weight)\n","        nn.init.xavier_uniform_(self.d_freq_h.weight)\n","        nn.init.xavier_uniform_(self.y_freq_h.weight)\n","        nn.init.xavier_uniform_(self.m_freq_t.weight)\n","        nn.init.xavier_uniform_(self.d_freq_t.weight)\n","        nn.init.xavier_uniform_(self.y_freq_t.weight)\n","\n","        nn.init.xavier_uniform_(self.m_phi_h.weight)\n","        nn.init.xavier_uniform_(self.d_phi_h.weight)\n","        nn.init.xavier_uniform_(self.y_phi_h.weight)\n","        nn.init.xavier_uniform_(self.m_phi_t.weight)\n","        nn.init.xavier_uniform_(self.d_phi_t.weight)\n","        nn.init.xavier_uniform_(self.y_phi_t.weight)\n","\n","        nn.init.xavier_uniform_(self.m_amps_h.weight)\n","        nn.init.xavier_uniform_(self.d_amps_h.weight)\n","        nn.init.xavier_uniform_(self.y_amps_h.weight)\n","        nn.init.xavier_uniform_(self.m_amps_t.weight)\n","        nn.init.xavier_uniform_(self.d_amps_t.weight)\n","        nn.init.xavier_uniform_(self.y_amps_t.weight)\n","\n","    def get_time_embedd(self, entities, years, months, days, h_or_t):\n","        if h_or_t == \"head\":\n","            emb  = self.y_amps_h(entities) * self.time_nl(self.y_freq_h(entities) * years  + self.y_phi_h(entities))\n","            emb += self.m_amps_h(entities) * self.time_nl(self.m_freq_h(entities) * months + self.m_phi_h(entities))\n","            emb += self.d_amps_h(entities) * self.time_nl(self.d_freq_h(entities) * days   + self.d_phi_h(entities))\n","        else:\n","            emb  = self.y_amps_t(entities) * self.time_nl(self.y_freq_t(entities) * years  + self.y_phi_t(entities))\n","            emb += self.m_amps_t(entities) * self.time_nl(self.m_freq_t(entities) * months + self.m_phi_t(entities))\n","            emb += self.d_amps_t(entities) * self.time_nl(self.d_freq_t(entities) * days   + self.d_phi_t(entities))\n","            \n","        return emb\n","    def create_adjacency_matrix(self, input_tensor, num_entities):\n","\n","        device = input_tensor.device\n","        \n","        heads = input_tensor[:, 0].long().to(device)  \n","        tails = input_tensor[:, 2].long().to(device) \n","        indices = torch.stack([heads, tails], dim=0).to(device)  \n","    \n","        values = torch.ones(indices.shape[1], dtype=torch.float32).to(device)\n","    \n","        adj = torch.sparse_coo_tensor(indices, values, (num_entities, num_entities)).to(device)\n","\n","        adj += torch.sparse_coo_tensor(\n","            torch.arange(num_entities).unsqueeze(0).repeat(2, 1).to(device), \n","            torch.ones(num_entities, dtype=torch.float32).to(device), \n","            (num_entities, num_entities)\n","        ).to(device)\n","    \n","\n","        degree = torch.sparse.sum(adj, dim=1).to_dense().to(device)  \n","        degree_inv = 1.0 / degree\n","        degree_inv[degree == 0] = 0  \n","    \n","        values = values * degree_inv[heads].to(device)\n","        adj = torch.sparse_coo_tensor(indices, values, (num_entities, num_entities)).to(device)\n","    \n","        return adj\n","\n","    def get_graph_embedd(self, heads, rels, tails, alldata):\n","\n","\n","        batch = torch.stack((heads, rels, tails), dim=1)\n","        alldata = alldata[:, :3] \n","        combine_batch_alldata = torch.cat((batch,alldata),dim = 0) \n","\n","        start = time.time()\n","        adjmatrix = self.create_adjacency_matrix(combine_batch_alldata,self.dataset.numEnt())\n","        end = time.time()\n","\n","        representGraph = self.graphRepresent(combine_batch_alldata, adjmatrix.cuda())\n","\n","        return representGraph[:batch.shape[0]]\n","    \n","\n","    def getEmbeddings(self, heads, rels, tails, years, months, days, alldata):\n","        years = years.view(-1,1)\n","        months = months.view(-1,1)\n","        days = days.view(-1,1)\n","        h_embs1 = self.ent_embs_h(heads)\n","        r_embs1 = self.rel_embs_f(rels)\n","        t_embs1 = self.ent_embs_t(tails)\n","        h_embs2 = self.ent_embs_h(tails)\n","        r_embs2 = self.rel_embs_i(rels)\n","        t_embs2 = self.ent_embs_t(heads)\n","        \n","        h_embs1 = torch.cat((h_embs1, self.get_time_embedd(heads, years, months, days, \"head\"),self.get_graph_embedd(heads, rels, tails, alldata)), 1)\n","        t_embs1 = torch.cat((t_embs1, self.get_time_embedd(tails, years, months, days, \"tail\"),self.get_graph_embedd(heads, rels, tails, alldata)), 1)\n","        h_embs2 = torch.cat((h_embs2, self.get_time_embedd(tails, years, months, days, \"head\"),self.get_graph_embedd(heads, rels, tails, alldata)), 1)\n","        t_embs2 = torch.cat((t_embs2, self.get_time_embedd(heads, years, months, days, \"tail\"),self.get_graph_embedd(heads, rels, tails, alldata)), 1)\n","        \n","        return h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2\n","    \n","    def forward(self, heads, rels, tails, years, months, days,alldata):\n","        h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2 = self.getEmbeddings(heads, rels, tails, years, months, days,alldata)\n","        scores = ((h_embs1 * r_embs1) * t_embs1 + (h_embs2 * r_embs2) * t_embs2) / 2.0\n","        scores = F.dropout(scores, p=self.params.dropout, training=self.training)\n","        scores = torch.sum(scores, dim=1)\n","        return scores\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["# G_simple.py"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.777210Z","iopub.status.busy":"2024-10-21T20:28:47.776875Z","iopub.status.idle":"2024-10-21T20:28:47.811733Z","shell.execute_reply":"2024-10-21T20:28:47.810668Z","shell.execute_reply.started":"2024-10-21T20:28:47.777174Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","\n","class G_SimplE(torch.nn.Module):\n","    def __init__(self, dataset, params):\n","        super(G_SimplE, self).__init__()\n","        self.dataset = dataset\n","        self.params = params\n","\n","        self.ent_embs_h = nn.Embedding(dataset.numEnt(), params.s_emb_dim)\n","        self.ent_embs_t = nn.Embedding(dataset.numEnt(), params.s_emb_dim)\n","        self.rel_embs_f = nn.Embedding(dataset.numRel(), params.s_emb_dim + params.t_emb_dim)\n","        self.rel_embs_i = nn.Embedding(dataset.numRel(), params.s_emb_dim + params.t_emb_dim)\n","\n","        nn.init.xavier_uniform_(self.ent_embs_h.weight)\n","        nn.init.xavier_uniform_(self.ent_embs_t.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_f.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_i.weight)\n","\n","        self.graphRepresent = GraphRepresentation(dataset.numEnt(), dataset.numRel(), params.t_emb_dim)\n","\n","    def getGraphBeforeTime(self, Current_Year, Current_Month, Current_Day, alldata): \n","        alldata = alldata.cuda() if not alldata.is_cuda else alldata\n","\n","        current_date = torch.tensor([Current_Year, Current_Month, Current_Day], device=alldata.device)\n","\n","        data_dates = alldata[:, 3:]\n","\n","        before_mask = (\n","            (data_dates[:, 0] < current_date[0]) |\n","            ((data_dates[:, 0] == current_date[0]) & (data_dates[:, 1] < current_date[1])) |\n","            ((data_dates[:, 0] == current_date[0]) & (data_dates[:, 1] == current_date[1]) & (data_dates[:, 2] < current_date[2]))\n","        )\n","\n","        filtered_data = alldata[before_mask]\n","\n","        return filtered_data\n","\n","\n","    def create_adjacency_matrix(self, input_tensor, num_entities):\n","\n","        device = input_tensor.device\n","        \n","        heads = input_tensor[:, 0].long().to(device)  # Cột đầu tiên là heads\n","        tails = input_tensor[:, 2].long().to(device)  # Cột thứ ba là tails\n","        \n","        indices = torch.stack([heads, tails], dim=0).to(device)  \n","    \n","        values = torch.ones(indices.shape[1], dtype=torch.float32).to(device)\n","    \n","        adj = torch.sparse_coo_tensor(indices, values, (num_entities, num_entities)).to(device)\n","    \n","        adj += torch.sparse_coo_tensor(\n","            torch.arange(num_entities).unsqueeze(0).repeat(2, 1).to(device), \n","            torch.ones(num_entities, dtype=torch.float32).to(device), \n","            (num_entities, num_entities)\n","        ).to(device)\n","    \n","        degree = torch.sparse.sum(adj, dim=1).to_dense().to(device) \n","        \n","        degree_inv = 1.0 / degree\n","        degree_inv[degree == 0] = 0 \n","        values = values * degree_inv[heads].to(device)\n","        adj = torch.sparse_coo_tensor(indices, values, (num_entities, num_entities)).to(device)\n","    \n","        return adj\n","\n","\n","    \n","    def get_graph_embedd(self, heads, rels, tails, years, months, days, alldata):\n","\n","\n","        batch = torch.stack((heads, rels, tails), dim=1) # get batch \n","        alldata = alldata[:, :3] # get only head , realtion and tail\n","        combine_batch_alldata = torch.cat((batch,alldata),dim = 0) #  add bactch and alldata for Graph Representation\n","        adjmatrix = self.create_adjacency_matrix(combine_batch_alldata,self.dataset.numEnt())\n","        representGraph = self.graphRepresent(combine_batch_alldata, adjmatrix.cuda())\n","      \n","        return representGraph[:batch.shape[0]]\n","\n","\n","    \n","\n","    def getEmbeddings(self, heads, rels, tails, years, months, days, alldata):\n","        years = years.view(-1, 1)\n","        months = months.view(-1, 1)\n","        days = days.view(-1, 1)\n","\n","        h_embs1 = self.ent_embs_h(heads)\n","        r_embs1 = self.rel_embs_f(rels)\n","        t_embs1 = self.ent_embs_t(tails)\n","        h_embs2 = self.ent_embs_h(tails)\n","        r_embs2 = self.rel_embs_i(rels)\n","        t_embs2 = self.ent_embs_t(heads)\n","\n","\n","        h_embs1 = torch.cat((h_embs1, self.get_graph_embedd(heads, rels, tails, years, months, days, alldata)), 1)\n","        t_embs1 = torch.cat((t_embs1, self.get_graph_embedd(tails, rels, heads, years, months, days, alldata)), 1)\n","        h_embs2 = torch.cat((h_embs2, self.get_graph_embedd(tails, rels, heads, years, months, days, alldata)), 1)\n","        t_embs2 = torch.cat((t_embs2, self.get_graph_embedd(heads, rels, tails, years, months, days, alldata)), 1)\n","\n","        return h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2\n","\n","    def forward(self, heads, rels, tails, years, months, days, alldata):\n","        h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2 = self.getEmbeddings(heads, rels, tails, years, months, days, alldata)\n","        scores = ((h_embs1 * r_embs1) * t_embs1 + (h_embs2 * r_embs2) * t_embs2) / 2.0\n","        scores = F.dropout(scores, p=self.params.dropout, training=self.training)\n","        scores = torch.sum(scores, dim=1)\n","        return scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# DE_simple.py"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.813454Z","iopub.status.busy":"2024-10-21T20:28:47.813132Z","iopub.status.idle":"2024-10-21T20:28:47.935833Z","shell.execute_reply":"2024-10-21T20:28:47.934767Z","shell.execute_reply.started":"2024-10-21T20:28:47.813421Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","\n","class DE_SimplE(torch.nn.Module):\n","    def __init__(self, dataset, params):\n","        super(DE_SimplE, self).__init__()\n","        self.dataset = dataset\n","        self.params = params\n","        \n","        self.ent_embs_h = nn.Embedding(dataset.numEnt(), params.s_emb_dim).cuda()\n","        self.ent_embs_t = nn.Embedding(dataset.numEnt(), params.s_emb_dim).cuda()\n","\n","        self.rel_embs_f = nn.Embedding(dataset.numRel(), params.s_emb_dim+params.t_emb_dim).cuda()\n","        self.rel_embs_i = nn.Embedding(dataset.numRel(), params.s_emb_dim+params.t_emb_dim).cuda()\n","        \n","        self.create_time_embedds()\n","\n","        self.time_nl = torch.sin  \n","        \n","        nn.init.xavier_uniform_(self.ent_embs_h.weight)\n","        nn.init.xavier_uniform_(self.ent_embs_t.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_f.weight)\n","        nn.init.xavier_uniform_(self.rel_embs_i.weight)\n","    \n","    def create_time_embedds(self):\n","\n","        self.m_freq_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.m_freq_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_freq_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_freq_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_freq_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_freq_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        self.m_phi_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.m_phi_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_phi_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_phi_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_phi_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_phi_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        self.m_amps_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.m_amps_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_amps_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_amps_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_amps_h = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_amps_t = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        nn.init.xavier_uniform_(self.m_freq_h.weight)\n","        nn.init.xavier_uniform_(self.d_freq_h.weight)\n","        nn.init.xavier_uniform_(self.y_freq_h.weight)\n","        nn.init.xavier_uniform_(self.m_freq_t.weight)\n","        nn.init.xavier_uniform_(self.d_freq_t.weight)\n","        nn.init.xavier_uniform_(self.y_freq_t.weight)\n","\n","        nn.init.xavier_uniform_(self.m_phi_h.weight)\n","        nn.init.xavier_uniform_(self.d_phi_h.weight)\n","        nn.init.xavier_uniform_(self.y_phi_h.weight)\n","        nn.init.xavier_uniform_(self.m_phi_t.weight)\n","        nn.init.xavier_uniform_(self.d_phi_t.weight)\n","        nn.init.xavier_uniform_(self.y_phi_t.weight)\n","\n","        nn.init.xavier_uniform_(self.m_amps_h.weight)\n","        nn.init.xavier_uniform_(self.d_amps_h.weight)\n","        nn.init.xavier_uniform_(self.y_amps_h.weight)\n","        nn.init.xavier_uniform_(self.m_amps_t.weight)\n","        nn.init.xavier_uniform_(self.d_amps_t.weight)\n","        nn.init.xavier_uniform_(self.y_amps_t.weight)\n","\n","    def get_time_embedd(self, entities, years, months, days, h_or_t):\n","        if h_or_t == \"head\":\n","            emb  = self.y_amps_h(entities) * self.time_nl(self.y_freq_h(entities) * years  + self.y_phi_h(entities))\n","            emb += self.m_amps_h(entities) * self.time_nl(self.m_freq_h(entities) * months + self.m_phi_h(entities))\n","            emb += self.d_amps_h(entities) * self.time_nl(self.d_freq_h(entities) * days   + self.d_phi_h(entities))\n","        else:\n","            emb  = self.y_amps_t(entities) * self.time_nl(self.y_freq_t(entities) * years  + self.y_phi_t(entities))\n","            emb += self.m_amps_t(entities) * self.time_nl(self.m_freq_t(entities) * months + self.m_phi_t(entities))\n","            emb += self.d_amps_t(entities) * self.time_nl(self.d_freq_t(entities) * days   + self.d_phi_t(entities))\n","            \n","        return emb\n","\n","    def getEmbeddings(self, heads, rels, tails, years, months, days, intervals = None):\n","        years = years.view(-1,1)\n","        months = months.view(-1,1)\n","        days = days.view(-1,1)\n","        h_embs1 = self.ent_embs_h(heads)\n","        r_embs1 = self.rel_embs_f(rels)\n","        t_embs1 = self.ent_embs_t(tails)\n","        h_embs2 = self.ent_embs_h(tails)\n","        r_embs2 = self.rel_embs_i(rels)\n","        t_embs2 = self.ent_embs_t(heads)\n","        \n","        h_embs1 = torch.cat((h_embs1, self.get_time_embedd(heads, years, months, days, \"head\")), 1)\n","        t_embs1 = torch.cat((t_embs1, self.get_time_embedd(tails, years, months, days, \"tail\")), 1)\n","        h_embs2 = torch.cat((h_embs2, self.get_time_embedd(tails, years, months, days, \"head\")), 1)\n","        t_embs2 = torch.cat((t_embs2, self.get_time_embedd(heads, years, months, days, \"tail\")), 1)\n","\n","        return h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2\n","    \n","    def forward(self, heads, rels, tails, years, months, days,alldata):\n","        h_embs1, r_embs1, t_embs1, h_embs2, r_embs2, t_embs2 = self.getEmbeddings(heads, rels, tails, years, months, days)\n","        scores = ((h_embs1 * r_embs1) * t_embs1 + (h_embs2 * r_embs2) * t_embs2) / 2.0\n","        scores = F.dropout(scores, p=self.params.dropout, training=self.training)\n","        scores = torch.sum(scores, dim=1)\n","        return scores\n","        \n"]},{"cell_type":"markdown","metadata":{},"source":["# DE_TransE.py"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.937556Z","iopub.status.busy":"2024-10-21T20:28:47.937170Z","iopub.status.idle":"2024-10-21T20:28:47.961082Z","shell.execute_reply":"2024-10-21T20:28:47.960105Z","shell.execute_reply.started":"2024-10-21T20:28:47.937518Z"},"trusted":true},"outputs":[],"source":["\n","\n","class DE_TransE(torch.nn.Module):\n","    def __init__(self, dataset, params):\n","        super(DE_TransE, self).__init__()\n","        self.dataset = dataset\n","        self.params = params\n","        \n","        self.ent_embs = nn.Embedding(dataset.numEnt(), params.s_emb_dim).cuda()\n","        self.rel_embs = nn.Embedding(dataset.numRel(), params.s_emb_dim+params.t_emb_dim).cuda()\n","        \n","        self.create_time_embedds()\n","        \n","        self.time_nl = torch.sin\n","        \n","        nn.init.xavier_uniform_(self.ent_embs.weight)\n","        nn.init.xavier_uniform_(self.rel_embs.weight)\n","        \n","        self.sigm = torch.nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","    \n","    def create_time_embedds(self):\n","            \n","        self.m_freq = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_freq = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_freq = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        nn.init.xavier_uniform_(self.m_freq.weight)\n","        nn.init.xavier_uniform_(self.d_freq.weight)\n","        nn.init.xavier_uniform_(self.y_freq.weight)\n","\n","        self.m_phi = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_phi = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_phi = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        nn.init.xavier_uniform_(self.m_phi.weight)\n","        nn.init.xavier_uniform_(self.d_phi.weight)\n","        nn.init.xavier_uniform_(self.y_phi.weight)\n","\n","        self.m_amp = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.d_amp = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","        self.y_amp = nn.Embedding(self.dataset.numEnt(), self.params.t_emb_dim).cuda()\n","\n","        nn.init.xavier_uniform_(self.m_amp.weight)\n","        nn.init.xavier_uniform_(self.d_amp.weight)\n","        nn.init.xavier_uniform_(self.y_amp.weight)\n","\n","    def get_time_embedd(self, entities, year, month, day):\n","        \n","        y = self.y_amp(entities)*self.time_nl(self.y_freq(entities)*year + self.y_phi(entities))\n","        m = self.m_amp(entities)*self.time_nl(self.m_freq(entities)*month + self.m_phi(entities))\n","        d = self.d_amp(entities)*self.time_nl(self.d_freq(entities)*day + self.d_phi(entities))\n","        \n","        return y+m+d\n","\n","    def getEmbeddings(self, heads, rels, tails, years, months, days, intervals = None):\n","        years = years.view(-1,1)\n","        months = months.view(-1,1)\n","        days = days.view(-1,1)\n","\n","        h,r,t = self.ent_embs(heads), self.rel_embs(rels), self.ent_embs(tails)\n","        \n","        h_t = self.get_time_embedd(heads, years, months, days)\n","        t_t = self.get_time_embedd(tails, years, months, days)\n","        \n","        h = torch.cat((h,h_t), 1)\n","        t = torch.cat((t,t_t), 1)\n","        return h,r,t\n","    \n","    def forward(self, heads, rels, tails, years, months, days,alldata):\n","        h_embs, r_embs, t_embs = self.getEmbeddings(heads, rels, tails, years, months, days)\n","        \n","        scores = h_embs + r_embs - t_embs\n","        scores = F.dropout(scores, p=self.params.dropout, training=self.training)\n","        scores = -torch.norm(scores, dim = 1)\n","        return scores\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# tester.py"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:28:47.962735Z","iopub.status.busy":"2024-10-21T20:28:47.962414Z","iopub.status.idle":"2024-10-21T20:28:47.979169Z","shell.execute_reply":"2024-10-21T20:28:47.978249Z","shell.execute_reply.started":"2024-10-21T20:28:47.962701Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","class Tester:\n","    def __init__(self, dataset, model_path, valid_or_test):\n","        self.model = torch.load(model_path)\n","        self.model.eval()\n","        self.dataset = dataset\n","        self.valid_or_test = valid_or_test\n","        self.measure = Measure()\n","        \n","        self.data_tensor = torch.tensor(self.dataset.data[self.valid_or_test]).to(next(self.model.parameters()).device)\n","        self.data_tensor = torch.tensor(self.dataset.data[\"train\"]).to(next(self.model.parameters()).device)\n","\n","    def getRank(self, sim_scores):  \n","        return (sim_scores > sim_scores[0]).sum() + 1\n","    \n","    def replaceAndShred(self, fact, raw_or_fil, head_or_tail):\n","        head, rel, tail, years, months, days = fact\n","        num_entities = self.dataset.numEnt()\n","\n","        if head_or_tail == \"head\":\n","            ret_facts = [(i, rel, tail, years, months, days) for i in range(num_entities)]\n","        else:  # head_or_tail == \"tail\"\n","            ret_facts = [(head, rel, i, years, months, days) for i in range(num_entities)]\n","        \n","        if raw_or_fil == \"raw\":\n","            ret_facts = [fact] + ret_facts\n","        else:  # raw_or_fil == \"fil\"\n","            ret_facts = [fact] + list(set(ret_facts) - self.dataset.all_facts_as_tuples)\n","\n","        return shredFacts(np.array(ret_facts))\n","    \n","    def test(self):\n","\n","        for i, fact in enumerate(self.dataset.data[self.valid_or_test]):\n","            settings = [\"fil\"]\n","\n","            for raw_or_fil in settings:\n","                for head_or_tail in [\"head\", \"tail\"]:\n","                    heads, rels, tails, years, months, days = self.replaceAndShred(fact, raw_or_fil, head_or_tail)\n","\n","                    sim_scores = self.model(heads, rels, tails, years, months, days, self.data_tensor).cpu().data.numpy()\n","                    \n","                    rank = self.getRank(sim_scores)\n","                    self.measure.update(rank, raw_or_fil)\n","\n","        self.measure.print_()\n","        print(\"~~~~~~~~~~~~~\")\n","        self.measure.normalize(len(self.dataset.data[self.valid_or_test]))\n","        self.measure.print_()\n","        \n","        return self.measure.mrr[\"fil\"]\n"]},{"cell_type":"markdown","metadata":{},"source":["# trainer.py"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:33:05.279414Z","iopub.status.busy":"2024-10-21T20:33:05.278733Z","iopub.status.idle":"2024-10-21T20:33:05.296208Z","shell.execute_reply":"2024-10-21T20:33:05.295264Z","shell.execute_reply.started":"2024-10-21T20:33:05.279374Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","loss_glo = []\n","model_path_glo = \"\"\n","\n","class Trainer:\n","    def __init__(self, dataset, params, model_name):\n","        instance_gen = globals()[model_name]\n","        self.model_name = model_name\n","        self.model = nn.DataParallel(instance_gen(dataset=dataset, params=params))\n","\n","        self.dataset = dataset\n","        self.params = params\n","        \n","    def train(self, early_stop=False):\n","        self.model.train()\n","        \n","        optimizer = torch.optim.Adam(\n","            self.model.parameters(), \n","            lr=self.params.lr, \n","            weight_decay=self.params.reg_lambda\n","        ) \n","        loss_f = nn.CrossEntropyLoss()\n","        \n","        for epoch in range(1, self.params.ne + 1):\n","            last_batch = False\n","            total_loss = 0.0\n","            start = time.time()\n","\n","            c_batch = 0\n","            while not last_batch:\n","                # c_batch += 1\n","             \n","                optimizer.zero_grad()\n","                \n","                (heads, rels, tails, years, months, days),alldata = self.dataset.nextBatch(self.params.bsize, neg_ratio=self.params.neg_ratio)\n","                last_batch = self.dataset.wasLastBatch()\n","\n","                scores = self.model(heads, rels, tails, years, months, days,alldata.cuda())\n","               \n","                ###Added for softmax####\n","                num_examples = int(heads.shape[0] / (1 + self.params.neg_ratio))\n","                scores_reshaped = scores.view(num_examples, self.params.neg_ratio+1)\n","                l = torch.zeros(num_examples).long().cuda()\n","                loss = loss_f(scores_reshaped, l)\n","                loss.backward()\n","                optimizer.step()\n","                total_loss += loss.cpu().item()\n","                \n","            print(time.time() - start)\n","            \n","            print(\"Loss in iteration \" + str(epoch) + \": \" + str(total_loss) + \"(\" + self.model_name + \",\" + self.dataset.name + \")\")\n","            \n","            if epoch % self.params.save_each == 0:\n","                self.saveModel(epoch)\n","            global loss_glo\n","            loss_glo.append(total_loss)\n","            \n","    def saveModel(self, chkpnt):\n","        print(\"Saving the model\")\n","        directory = \"/kaggle/working/models\"\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","        global model_path_glo\n","        model_path_glo = directory + self.params.str_() + \"_\" + str(chkpnt) + \".chkpnt\"\n","        \n","            \n","        torch.save(self.model, directory + self.params.str_() + \"_\" + str(chkpnt) + \".chkpnt\")\n","        \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# main.py"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T20:29:39.342306Z","iopub.status.busy":"2024-10-21T20:29:39.341507Z","iopub.status.idle":"2024-10-21T20:29:53.115798Z","shell.execute_reply":"2024-10-21T20:29:53.114308Z","shell.execute_reply.started":"2024-10-21T20:29:39.342252Z"},"trusted":true},"outputs":[],"source":["\n","bs = 1024\n","neg_ratio = 150\n","se_prop = 0.36\n","emb_dim = 100\n","ne = 50\n","save_each = 50\n","model_ = 'G_DE_SimplE'  # 'DE_TransE', 'DE_SimplE','G_DE_SimplE'\n","expDataset = '/kaggle/input/dataknowledgegraph/datasets/icews14/'\n","\n","\n","\n","\n","\n","class Args:\n","    def __init__(self):\n","        self.dataset = expDataset\n","        self.model = model_\n","        self.ne = ne\n","        self.bsize = bs\n","        self.lr = 0.001\n","        self.reg_lambda = 0.0\n","        self.emb_dim = emb_dim\n","        self.neg_ratio = neg_ratio\n","        self.dropout = 0.4\n","        self.save_each = save_each\n","        self.se_prop = se_prop\n","\n","\n","args = Args()\n","dataset = Dataset(args.dataset)\n","params = Params(\n","    ne=args.ne, \n","    bsize=args.bsize, \n","    lr=args.lr, \n","    reg_lambda=args.reg_lambda, \n","    emb_dim=args.emb_dim, \n","    neg_ratio=args.neg_ratio, \n","    dropout=args.dropout, \n","    save_each=args.save_each, \n","    se_prop=args.se_prop\n",")\n","\n","\n","print(\"Start training\")\n","trainer = Trainer(dataset, params, args.model)\n","trainer.train()\n","\n","print(\"start testing\")\n","model_path = model_path_glo\n","tester = Tester(dataset, model_path, \"test\")\n","tester.test()\n","\n","\n","\n","# Save inf model\n","def printInf():\n","\n","    print(\"Hyperparameter: \")\n","    print(\"Number epoch (ne):\", params.ne)\n","    print(\"Batch Size (bsize):\", params.bsize)\n","\n","    print(\"Embedding Dimension (emb_dim):\", params.emb_dim)\n","    print(\"Negative Ratio (neg_ratio):\", params.neg_ratio)\n","\n","    print(\"SE Property (se_prop):\", params.se_prop)\n","    print(\"\")\n","    \n","    print(\"Loss:\")\n","    print(loss_glo)\n","    print(\"\")\n","\n","    print(\"Test\")\n","    tester.measure.print__()\n","printInf()\n","\n","file_name = f\"ne_{params.ne}_bsize_{params.bsize}_embdim_{params.emb_dim}_neg_{params.neg_ratio}_seprop_{params.se_prop}.txt\"\n","file_path = f\"/kaggle/working/exp/{file_name}\"\n","\n","\n","os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","with open(file_path, 'w') as f:\n","    with contextlib.redirect_stdout(f):\n","        printInf()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5916549,"sourceId":9679975,"sourceType":"datasetVersion"},{"datasetId":5921080,"sourceId":9686015,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":4}
